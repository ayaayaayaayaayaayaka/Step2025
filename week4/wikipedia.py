import sys
import collections
from collections import deque

class Wikipedia:

    # Initialize the graph of pages.
    def __init__(self, pages_file, links_file):

        # A mapping from a page ID (integer) to the page title.
        # For example, self.titles[1234] returns the title of the page whose
        # ID is 1234.
        self.titles = {} # IDã¨titleã‚’ç´ä»˜ã‘

        # A set of page links.
        # For example, self.links[1234] returns an array of page IDs linked
        # from the page whose ID is 1234.
        self.links = {} # IDã¨ãƒªãƒ³ã‚¯ã‚’ç´ä»˜ã‘

        # Read the pages file into self.titles.
        with open(pages_file) as file:
            for line in file:
                (id, title) = line.rstrip().split(" ") #ã€€ç©ºç™½ãŒäºŒã¤ä»¥ä¸Šã‚ã£ãŸã‚‰ã‚„ã°ããªã„ã‹ï¼Ÿ -> ãªã•ãã†
                id = int(id)
                assert not id in self.titles, id # assert <æ¡ä»¶å¼>,<æ¡ä»¶ãŒæº€ãŸã•ã‚Œãªã‹ã£ãŸå ´åˆã«è¡¨ç¤ºã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸>
                self.titles[id] = title
                self.links[id] = []
        print("Finished reading %s" % pages_file)

        self.inverted_titles = {}
        for id in self.titles:
            title = self.titles[id] 
            self.inverted_titles[title] = id

        # Read the links file into self.links.
        with open(links_file) as file:
            for line in file:
                (src, dst) = line.rstrip().split(" ")
                (src, dst) = (int(src), int(dst))
                assert src in self.titles, src
                assert dst in self.titles, dst
                self.links[src].append(dst)
        print("Finished reading %s" % links_file)
        print()

        # self.linksã§ãƒªãƒ³ã‚¯ãŒãªã„ãƒšãƒ¼ã‚¸ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯ã™ã¹ã


    # Example: Find the longest titles.
    def find_longest_titles(self):
        titles = sorted(self.titles.values(), key=len, reverse=True)
        print("The longest titles are:")
        count = 0
        index = 0
        while count < 15 and index < len(titles):
            if titles[index].find("_") == -1:
                print(titles[index])
                count += 1
            index += 1
        print()


    # Example: Find the most linked pages.
    def find_most_linked_pages(self):
        link_count = {}
        for id in self.titles.keys():
            link_count[id] = 0

        for id in self.titles.keys():
            for dst in self.links[id]:
                link_count[dst] += 1

        print("The most linked pages are:")
        link_count_max = max(link_count.values())
        for dst in link_count.keys():
            if link_count[dst] == link_count_max:
                print(self.titles[dst], link_count_max)
        print()


    # Homework #1: Find the shortest path.
    # 'start': A title of the start page.
    # 'goal': A title of the goal page.
    # find_shortest_path() é–¢æ•°ã‚’æ›¸ã„ã¦ã€ã‚ã‚‹ãƒšãƒ¼ã‚¸ã‹ã‚‰åˆ¥ã®ãƒšãƒ¼ã‚¸ã¸ã®æœ€çŸ­çµŒè·¯ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ğŸ˜€
    # ã€Œæ¸‹è°·ã€ã‹ã‚‰ã€Œå°é‡å¦¹å­ã€ã«ã©ã†ãŸã©ã‚Šç€ãï¼Ÿ
    # ãƒ’ãƒ³ãƒˆï¼š
    # BFS ã«å·¥å¤«ã‚’å…¥ã‚Œã¦æœ€çŸ­çµŒè·¯ã‚’å‡ºã›ã‚‹ã‚ˆã†ã«ã™ã‚‹
    # collections.deque ã‚’ä½¿ã†ã¨ã‚¹ã‚¿ãƒƒã‚¯ã‚„ã‚­ãƒ¥ãƒ¼ãŒä½œã‚Œã¾ã™
    # 30 è¡Œç¨‹åº¦ã§æ›¸ã‘ã¾ã™ğŸ˜€

    def find_shortest_path(self, start, goal):
        # ãŸã¨ãˆã°ã€pathãŒä¸€ç›´ç·šã ã£ãŸã‚‰
        # æ™‚é–“ã‚‚ç©ºé–“ã‚‚ä¼¸ã³ã¦ã€n^2ã«ãªã‚‹
        # "pathã‚’å¾©å…ƒã™ã‚‹"
        # è·é›¢ã‚’è¨ˆç®—ã—ã¦ãã‚Œã‚’ä¿å­˜ã™ã‚‹ -> O(N + E)
        # çµ‚ç‚¹ã‹ã‚‰é€†ã‚’è¾¿ã‚‹
        # å¤‰ã®å‘ãã‚’é€†ã«ã—ãŸã‚°ãƒ©ãƒ•ã‚’è€ƒãˆã€æœ€çŸ­è·é›¢-1ã‚’ã¾ãŸã‹ã‚“ãŒãˆã€ã“ã‚Œã‚’ç¹°ã‚Šè¿”ã™ã¨å…ƒã«æˆ»ã‚Œã‚‹
        # è¡Œãã‚‚å¸°ã‚Šã‚‚O(N+E)ã§å®Ÿè¡Œã§ãã‚‹
        queue = deque() # æ¢ç´¢ä¸­ã®ãƒãƒ¼ãƒ‰é›†åˆ
        seen = set() # è¨ªå•æ¸ˆã¿idé›†åˆ
        start_id = self.inverted_titles[start] # start titleã‚’idã«
        goal_id = self.inverted_titles[goal] # goal titleã‚’idã«
        queue.append([start_id]) # å§‹ç‚¹ã‚’queueã«è¿½åŠ 
        seen.add(start_id) # å§‹ç‚¹ã«ã¯è¨ªå•æ¸ˆã¿
        while len(queue):
            curr = queue.popleft() # æ¢ç´¢ãƒãƒ¼ãƒ‰ã‚’å–ã‚Šå‡ºã™
            if curr[-1] == goal_id: # goalåˆ¤å®š
                path = [self.titles[id] for id in reversed(curr)] # å§‹ç‚¹ã‹ã‚‰çµ‚ç‚¹ã¸ã¨ä¸¦ã³æ›¿ãˆ
                print(f"shortest path : {path}")
                return
            for des in self.links[curr[-1]]: # ãƒãƒ¼ãƒ‰ã®è¡Œãå…ˆã‚’èª¿ã¹ã‚‹
                if not des in seen:
                    new_path = curr.copy() # queueã«æ ¼ç´ã™ã‚‹æ–°ãŸãªlist
                    # listã®é•·ã•ã¶ã‚“æ™‚é–“ãŒã‹ã‹ã‚‹
                    new_path.append(des) # listã®æœ«å°¾ã«è¡Œãå…ˆã‚’è¿½åŠ 
                    queue.append(new_path) # listæ ¼ç´
                    seen.add(des) # è¨ªå•æ¸ˆã¿idã«è¿½åŠ 
        print("not found")
        return
    


    # Homework #2: Calculate the page ranks and print the most popular pages.
    # æ­£ã—ã•ã®ç¢ºèªæ–¹æ³•
    # ãƒšãƒ¼ã‚¸ãƒ©ãƒ³ã‚¯ã®åˆ†é…ã¨æ›´æ–°ã‚’ä½•å›ç¹°ã‚Šè¿”ã—ã¦ã‚‚ã€Œå…¨éƒ¨ã®ãƒãƒ¼ãƒ‰ã®ãƒšãƒ¼ã‚¸ãƒ©ãƒ³ã‚¯ã®åˆè¨ˆå€¤ã€ãŒä¸€å®šã«ä¿ãŸã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„
    # ä¸€å®šã«ãªã‚‰ãªã„å ´åˆä½•ã‹ãŒé–“é•ã£ã¦ã¾ã™ï¼

    # Large ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å‹•ã‹ã™ãŸã‚ã«ã¯ O(N + E) ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå¿…è¦ã§ã™
    # ãƒšãƒ¼ã‚¸æ•°ï¼šN = 2215900
    # ãƒªãƒ³ã‚¯æ•°ï¼šE = 119006494

    # ãƒšãƒ¼ã‚¸ãƒ©ãƒ³ã‚¯ã®æ›´æ–°ãŒã€Œå®Œå…¨ã«ã€åæŸã™ã‚‹ã®ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã‚‹ã®ã§ã€æ›´æ–°ãŒååˆ†å°‘ãªããªã£ãŸã‚‰æ­¢ã‚ã‚‹

    # åæŸæ¡ä»¶ã®ä½œã‚Šæ–¹ã®ä¾‹ï¼š
    # âˆ‘(new_pagerank[i] - old_pagerank[i])^2 < 0.01

    def find_most_popular_pages(self):
        pagerank = {} # {id:pagerank}
        new_pagerank = {} # åˆæœŸåŒ–
        for id in self.links: # O(N)
            pagerank[id] = 1.0 # åˆæœŸåŒ–
        memo = {}
        N = len(self.links) # Nodeæ•°
        loop = 0 # loopã‚«ã‚¦ãƒ³ãƒˆ
        diff = 10000
        result = [] 
        sum_pageranks = 0

        while diff > 0.01: # å¸¸ã«Trueãªã‚‰Trueã£ã¦æ›¸ã‘ã°è‰¯ã„
            res = 0 # å…¨ãƒãƒ¼ãƒ‰å…±é€šã®è¶³ã—ç®—é …ã€åˆæœŸåŒ–
            diff = 0 # pagerankãŒãƒ«ãƒ¼ãƒ—ä¸€å›ã§ã©ã‚Œãã‚‰ã„æ›´æ–°ã•ã‚ŒãŸã‹ã€åˆæœŸåŒ–
            sum_pageranks = 0 # pagerankã®ç·å’Œã€åˆæœŸåŒ–
            # for id in self.links: # O(N)
            #     memo[id] = 0 # åˆæœŸåŒ–
            for id in self.links: # O(N)
                new_pagerank[id] = 0  # åˆæœŸåŒ–
            result = [] # çµæœãƒ¡ãƒ¢ç”¨ãƒªã‚¹ãƒˆ
            for id in self.links: # O(N+E)ã€€<- ä»Šã®ã‚³ãƒ¼ãƒ‰ã§ã‚ã‚Œã°æ­£ã—ã„ã€€æ–°ã—åŒºãã‚Œãã‚Œè¶³ã—å§‹ã‚ãŸã‚‰N^2 + E
                if len(self.links[id]) == 0: # è¡Œãæ­¢ã¾ã‚Šãƒãƒ¼ãƒ‰
                    res += pagerank[id] / N # ã‚ã¨ã§å…¨å“¡ä¸€æ–‰ã«æ›´æ–°ã™ã‚‹ã‚ˆã†ãƒ¡ãƒ¢
                else:
                    res += pagerank[id] * 0.15 / N # 0.15 -> ã‚ã¨ã§å…¨å“¡ä¸€æ–‰ã«æ›´æ–°ã™ã‚‹ã‚ˆã†ãƒ¡ãƒ¢
                    add = pagerank[id] * 0.85 / len(self.links[id]) # 0.85 -> è¿‘æ‰€ã«åˆ†ã‘ã‚‹
                    for neighbor in self.links[id]:
                        new_pagerank[neighbor] += add # è©²å½“ãƒãƒ¼ãƒ‰ã®pagerankã®ã¿æ›´æ–°
            for id in self.links: # O(N)
                diff += (new_pagerank[id] - pagerank[id] + res ) ** 2 # èª¤å·®é …
                pagerank[id] = new_pagerank[id] + res   # å…±é€šé …ã‚’ä¸€æ–‰ã«æ›´æ–°
                sum_pageranks += pagerank[id] # ç·å’Œè¨ˆç®—
                result.append((pagerank[id],id)) 
            loop += 1
            print(f"{loop}loop, diff : {diff}") # é€”ä¸­çµŒé(ãƒ«ãƒ¼ãƒ—æ•°,èª¤å·®)è¡¨ç¤º

        
        print("-----------------CONVERGED!-----------------")
        print(f"sum ( == {N} ã§ã‚ã‚Œã°è‰¯ã„): {sum_pageranks}") # sum == Nã«ãªã‚‹ã¯ãšã€å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        result.sort(reverse = True) # pagerankå¤§ãã„é †ã«ä¸¦ã³ã‹ãˆ
        top_ids = [tupl[1] for tupl in result[:10]] 
        top_titles = [(self.titles[id],pagerank[id]) for id in top_ids]
        print(f"top pagerank titles :{top_titles}")
        return
           



    # Homework #3 (optional):
    # Search the longest path with heuristics.
    # 'start': A title of the start page.
    # 'goal': A title of the goal page.
    # Wikipedia ã®ã‚°ãƒ©ãƒ•ã«ã¤ã„ã¦ã€Œæ¸‹è°·ã€ã‹ã‚‰ã€Œæ± è¢‹ã€ã¾ã§ã€åŒã˜ãƒšãƒ¼ã‚¸ã‚’é‡è¤‡ã—ã¦é€šã‚‰ãªã„ã€ã§ãã‚‹ã ã‘é•·ã„çµŒè·¯ã‚’ç™ºè¦‹ã—ã¦ãã ã•ã„ï¼ï¼
    def find_longest_path(self, start, goal):
        #------------------------#
        # Write your code here!  #
        #------------------------#
        pass


    # Helper function for Homework #3:
    # Please use this function to check if the found path is well formed.
    # 'path': An array of page IDs that stores the found path.
    #     path[0] is the start page. path[-1] is the goal page.
    #     path[0] -> path[1] -> ... -> path[-1] is the path from the start
    #     page to the goal page.
    # 'start': A title of the start page.
    # 'goal': A title of the goal page.
    def assert_path(self, path, start, goal):
        assert(start != goal)
        assert(len(path) >= 2)
        assert(self.titles[path[0]] == start)
        assert(self.titles[path[-1]] == goal)
        for i in range(len(path) - 1):
            assert(path[i + 1] in self.links[path[i]])


if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("usage: %s pages_file links_file" % sys.argv[0])
        exit(1)

    wikipedia = Wikipedia(sys.argv[1], sys.argv[2])
    # Example
    # wikipedia.find_longest_titles()
    # # Example
    # wikipedia.find_most_linked_pages()
    # # Homework #1
    wikipedia.find_shortest_path("æ¸‹è°·", "ãƒ‘ãƒ¬ãƒ¼ãƒˆã®æ³•å‰‡")
    # Homework #2
    wikipedia.find_most_popular_pages()
    # Homework #3 (optional)
    wikipedia.find_longest_path("æ¸‹è°·", "æ± è¢‹")

## python wikipedia.py wikipedia_dataset/pages_small.txt wikipedia_dataset/links_small.txt
## python wikipedia.py wikipedia_dataset/pages_medium.txt wikipedia_dataset/links_medium.txt
## python wikipedia.py wikipedia_dataset/pages_large.txt wikipedia_dataset/links_large.txt
